# AIStream æ–‡æ¡£

> **æ–‡ä»¶ä½ç½®**: `src/infrastructure/aistream/aistream.js`  
> Node ä¾§"å¤šæ­¥å·¥ä½œæµ/WorkflowManager/TODO"å·²ç§»é™¤ï¼›å¤æ‚å¤šæ­¥ç¼–æ’è¯·ä½¿ç”¨ Python å­æœåŠ¡ç«¯ï¼ˆLangChain/LangGraphï¼‰ã€‚æœ¬æ–‡æ¡£æè¿°çš„æ˜¯ Node ä¾§ `AIStream` åŸºç±»ä¸ LLM/MCP é›†æˆæ–¹å¼ã€‚
> **å¯æ‰©å±•æ€§**ï¼šAIStreamæ˜¯å·¥ä½œæµç³»ç»Ÿçš„æ ¸å¿ƒæ‰©å±•ç‚¹ã€‚é€šè¿‡ç»§æ‰¿AIStreamï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰å·¥ä½œæµã€‚è¯¦è§ **[æ¡†æ¶å¯æ‰©å±•æ€§æŒ‡å—](æ¡†æ¶å¯æ‰©å±•æ€§æŒ‡å—.md)** â­
> **ç›¸å…³æ–‡æ¡£**ï¼šå…³äº LLM/Vision/ASR/TTS å·¥å‚ç³»ç»Ÿçš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚è€ƒ **[å·¥å‚ç³»ç»Ÿæ–‡æ¡£](factory.md)** ğŸ“–

`AIStream` æ˜¯ XRK-AGT ä¸­çš„ **AI å·¥ä½œæµåŸºç±»**ï¼Œç”¨äºå°è£… LLM è°ƒç”¨ã€å‘é‡æœåŠ¡ã€ä¸Šä¸‹æ–‡å¢å¼ºç­‰èƒ½åŠ›ï¼ˆå·¥å…·è°ƒç”¨ç”± LLM å·¥å‚çš„ tool calling + MCP ç»Ÿä¸€å¤„ç†ï¼ŒAIStream æœ¬èº«**ä¸å†è§£æå‡½æ•°è°ƒç”¨æ–‡æœ¬**ï¼‰ã€‚

### æ‰©å±•ç‰¹æ€§

- âœ… **é›¶é…ç½®æ‰©å±•**ï¼šæ”¾ç½®åˆ°ä»»æ„ `core/*/stream/` ç›®å½•å³å¯è‡ªåŠ¨åŠ è½½
- âœ… **å‡½æ•°æ³¨å†Œç³»ç»Ÿ**ï¼šç»Ÿä¸€ä½¿ç”¨ MCP å·¥å…·æ³¨å†Œ
- âœ… **å‘é‡æœåŠ¡é›†æˆ**ï¼šç»Ÿä¸€é€šè¿‡å­æœåŠ¡ç«¯å‘é‡æœåŠ¡è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–å’Œæ£€ç´¢
- âœ… **å·¥ä½œæµåˆå¹¶**ï¼šæ”¯æŒåŠŸèƒ½åˆå¹¶å’Œç»„åˆ
- âœ… **ä¸Šä¸‹æ–‡å¢å¼º**ï¼šè‡ªåŠ¨ä¸Šä¸‹æ–‡æ£€ç´¢å’Œå¢å¼ºï¼ˆRAGæµç¨‹ï¼‰
- âœ… **çƒ­é‡è½½æ”¯æŒ**ï¼šä¿®æ”¹ä»£ç åè‡ªåŠ¨é‡è½½

æ‰€æœ‰è‡ªå®šä¹‰ AI å·¥ä½œæµéƒ½åº”ç»§æ‰¿æ­¤ç±»ï¼Œå¯é€‰æ‹©å®ç° `buildSystemPrompt` ä¸ `buildChatContext`ã€‚

---

## æ¶æ„æ¦‚è§ˆ

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
flowchart LR
    subgraph Plugin["ğŸ”Œ æ’ä»¶å±‚"]
        direction TB
        Call["ğŸ“ stream.process(e, question)<br/>è°ƒç”¨å·¥ä½œæµ"]
    end
    
    subgraph AIStream["ğŸŒŠ AIStreamåŸºç±»"]
        direction TB
        BuildCtx["ğŸ“ buildChatContext<br/>æ„å»ºåŸºç¡€æ¶ˆæ¯"]
        Enhance["ğŸ” buildEnhancedContext<br/>RAGæµç¨‹ï¼šæ£€ç´¢å†å²+çŸ¥è¯†åº“"]
        CallAI["ğŸ“¡ callAI/callAIStream<br/>è°ƒç”¨LLM"]
        Store["ğŸ’¾ storeMessageWithEmbedding<br/>å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ"]
        Register["ğŸ”§ registerMCPTool<br/>æ³¨å†ŒMCPå·¥å…·"]
    end
    
    subgraph Subserver["ğŸ Pythonå­æœåŠ¡ç«¯"]
        direction TB
        LangChain["ğŸŒ LangChainæœåŠ¡<br/>/api/langchain/chat<br/>Agentç¼–æ’+å·¥å…·è°ƒç”¨"]
        VectorAPI["ğŸ“Š å‘é‡æœåŠ¡<br/>/api/vector/*<br/>embed/search/upsert"]
    end
    
    subgraph MainServer["âš™ï¸ ä¸»æœåŠ¡ç«¯"]
        direction TB
        LLMFactory["ğŸ¤– LLMå·¥å‚<br/>gptgod/volcengine/xiaomimimo<br/>openai/gemini/anthropic<br/>azure_openai/openai_compat"]
        HTTPAPI["ğŸŒ HTTP API<br/>/api/v3/chat/completions<br/>/api/ai/stream"]
        MCP["ğŸ”§ MCPæœåŠ¡å™¨<br/>å·¥å…·è°ƒç”¨åè®®"]
    end
    
    subgraph Memory["ğŸ§  è®°å¿†ç³»ç»Ÿ"]
        direction TB
        ShortTerm["ğŸ“ çŸ­æœŸè®°å¿†<br/>MemoryManager"]
        LongTerm["ğŸ” é•¿æœŸè®°å¿†<br/>å‘é‡æ£€ç´¢"]
    end
    
    Plugin -->|"è°ƒç”¨"| AIStream
    AIStream -->|"ä¼˜å…ˆè°ƒç”¨"| LangChain
    LangChain -->|"è°ƒç”¨"| LLMFactory
    LangChain -->|"å·¥å…·è°ƒç”¨"| MCP
    AIStream -->|"å¤±è´¥å›é€€"| LLMFactory
    AIStream -->|"å‘é‡æ£€ç´¢"| VectorAPI
    AIStream -->|"å­˜å‚¨å‘é‡"| VectorAPI
    AIStream -->|"æ³¨å†Œå·¥å…·"| MCP
    AIStream -->|"è¯»å†™"| Memory
    HTTPAPI -->|"ä½¿ç”¨"| LLMFactory
    
    style Plugin fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style AIStream fill:#50C878,stroke:#3FA060,stroke-width:3px,color:#fff
    style Subserver fill:#FFA500,stroke:#CC8400,stroke-width:2px,color:#fff
    style MainServer fill:#FFD700,stroke:#CCAA00,stroke-width:3px,color:#000
    style Memory fill:#FFB6C1,stroke:#FF69B4,stroke-width:2px
    style MCP fill:#DDA0DD,stroke:#9370DB,stroke-width:2px
    style LLMFactory fill:#9B59B6,stroke:#7D3C98,stroke-width:2px,color:#fff
    style HTTPAPI fill:#3498DB,stroke:#2980B9,stroke-width:2px,color:#fff
```

### å·¥ä½œæµæ‰§è¡Œæµç¨‹å›¾

```mermaid
sequenceDiagram
    participant Plugin as ğŸ”Œ æ’ä»¶
    participant Stream as ğŸŒŠ AIStream
    participant Context as ğŸ“ ä¸Šä¸‹æ–‡æ„å»º
    participant Vector as ğŸ“Š å‘é‡æœåŠ¡
    participant LLM as ğŸ¤– LLMæœåŠ¡
    participant Memory as ğŸ§  è®°å¿†ç³»ç»Ÿ
    
    Note over Plugin,Memory: ğŸ”„ AIStream å·¥ä½œæµæ‰§è¡Œæµç¨‹
    
    Plugin->>Stream: ğŸ“ process(e, question, options)<br/>è°ƒç”¨å·¥ä½œæµ
    Stream->>Context: ğŸ“ buildChatContext(e, question)<br/>æ„å»ºåŸºç¡€æ¶ˆæ¯
    Context-->>Stream: âœ… baseMessages<br/>åŸºç¡€æ¶ˆæ¯æ•°ç»„
    
    alt ğŸ” å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º
        Stream->>Vector: ğŸ” retrieveRelevantContexts(groupId, query)<br/>æ£€ç´¢å†å²ä¸Šä¸‹æ–‡
        Vector->>Vector: ğŸ“¡ POST /api/vector/search<br/>å‘é‡æœç´¢
        Vector-->>Stream: ğŸ“‹ historyContexts<br/>å†å²ä¸Šä¸‹æ–‡
        Stream->>Stream: ğŸ” retrieveKnowledgeContexts(query)<br/>æ£€ç´¢çŸ¥è¯†åº“
        Stream-->>Stream: ğŸ“š knowledgeContexts<br/>çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        Stream->>Context: ğŸ”— buildEnhancedContext(e, question, baseMessages)<br/>æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
        Context-->>Stream: âœ¨ enhancedMessages<br/>å¢å¼ºåçš„æ¶ˆæ¯
    end
    
    Stream->>LLM: ğŸ“¡ callAI(messages, apiConfig)<br/>è°ƒç”¨LLM
    
    alt ğŸ å­æœåŠ¡ç«¯å¯ç”¨
        LLM->>LLM: ğŸŒ POST /api/langchain/chat<br/>LangChainç¼–æ’
        LLM->>LLM: ğŸ“¡ POST /api/v3/chat/completions<br/>è°ƒç”¨LLMå·¥å‚
        alt ğŸ”§ éœ€è¦å·¥å…·è°ƒç”¨
            LLM->>LLM: ğŸ”§ æ‰§è¡ŒMCPå·¥å…·<br/>å·¥å…·è°ƒç”¨
            LLM-->>LLM: âœ… å·¥å…·ç»“æœ
        end
        LLM-->>Stream: âœ… LLMå“åº”<br/>AIå›å¤æ–‡æœ¬
    else âš™ï¸ å­æœåŠ¡ç«¯ä¸å¯ç”¨
        Stream->>LLM: ğŸ“¡ ç›´æ¥è°ƒç”¨LLMFactory<br/>å·¥å‚æ¨¡å¼
        LLM-->>Stream: âœ… LLMå“åº”<br/>AIå›å¤æ–‡æœ¬
    end
    
    alt ğŸ’¾ å¯ç”¨è®°å¿†å­˜å‚¨
        Stream->>Memory: ğŸ’¾ storeMessageWithEmbedding(groupId, message)<br/>å­˜å‚¨æ¶ˆæ¯å’Œå‘é‡
        Memory->>Vector: ğŸ“¡ POST /api/vector/upsert<br/>ä¸Šä¼ å‘é‡
        Vector-->>Memory: âœ… å­˜å‚¨æˆåŠŸ
    end
    
    Stream-->>Plugin: âœ… response<br/>è¿”å›æœ€ç»ˆå“åº”
    
    Note over Plugin: âœ¨ å·¥ä½œæµæ‰§è¡Œå®Œæˆ
```

### ç»„ä»¶å…³ç³»å›¾

```mermaid
classDiagram
    class AIStream {
        +name: string
        +description: string
        +version: string
        +author: string
        +priority: number
        +config: Object
        +embeddingConfig: Object
        +mcpTools: Map
        +init()
        +initEmbedding()
        +buildSystemPrompt(context)
        +buildChatContext(e, question)
        +buildEnhancedContext(e, question, baseMessages)
        +generateEmbedding(text)
        +retrieveRelevantContexts(groupId, query)
        +retrieveKnowledgeContexts(query)
        +optimizeContexts(contexts, maxTokens)
        +callAI(messages, apiConfig)
        +callAIStream(messages, apiConfig, onDelta, options)
        +execute(e, question, config)
        +process(e, question, options)
        +resolveLLMConfig(apiConfig)
        +getProviderConfig(provider)
        +checkPermission(permission, context)
        +merge(stream, options)
        +autoMergeAuxiliaryStreams(stream, options)
        +extractStreamNames(options)
        +classifyError(error)
        +shouldRetry(errorInfo, retryConfig, attempt)
        +getRetryConfig()
        +calculateRetryDelay(attempt, retryConfig)
        +getTimeoutSeconds(config)
        +handleError(error, operation, context)
        +successResponse(data)
        +errorResponse(code, message)
        +getInfo()
        +cleanup()
    }
    
    class StreamLoader {
        +streams: Map
        +load()
        +getStream(name)
        +mergeStreams(options)
        +initMCP()
    }
    
    class MemoryManager {
        +addShortTermMemory(userId, memory)
        +addLongTermMemory(userId, memory)
        +searchLongTermMemories(userId, query, limit)
    }
    
    
    class MonitorService {
        +startTrace(traceId, context)
        +recordTokens(traceId, tokens)
        +endTrace(traceId, result)
    }
    
    AIStream --> StreamLoader : é€šè¿‡LoaderåŠ è½½
    AIStream --> MemoryManager : ä½¿ç”¨è®°å¿†ç³»ç»Ÿ
    AIStream --> MonitorService : ç›‘æ§è¿½è¸ª
    StreamLoader --> AIStream : ç®¡ç†å®ä¾‹
```

---

## æ„é€ å‚æ•°ä¸åŸºç¡€é…ç½®

```javascript
constructor(options = {})
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `name` | `string` | å·¥ä½œæµåç§° | `'base-stream'` |
| `description` | `string` | æè¿° | `'åŸºç¡€å·¥ä½œæµ'` |
| `version` | `string` | ç‰ˆæœ¬å· | `'1.0.0'` |
| `author` | `string` | ä½œè€…æ ‡è¯† | `'unknown'` |
| `priority` | `number` | å·¥ä½œæµä¼˜å…ˆçº§ | `100` |
| `config` | `Object` | AIè°ƒç”¨é…ç½® | `{ enabled: true, temperature: 0.8, ... }` |
| `embedding` | `Object` | Embeddingé…ç½® | `{ enabled: true, maxContexts: 5 }` |
| `functionToggles` | `Object` | å‡½æ•°å¼€å…³é…ç½® | `{}` |

**AIè°ƒç”¨é…ç½®** (`config`)ï¼š
- `enabled` - æ˜¯å¦å¯ç”¨ï¼ˆé»˜è®¤ `true`ï¼‰
- `temperature`ã€`maxTokens`ã€`topP`ã€`presencePenalty`ã€`frequencyPenalty` ç­‰
- è¿è¡Œæ—¶å¯åœ¨æ’ä»¶ä¸­é¢å¤–ä¼ å…¥ `apiConfig` è¦†ç›–éƒ¨åˆ†å­—æ®µ

### å…¨å±€é…ç½®

å·¥ä½œæµç³»ç»Ÿå…¨å±€é…ç½®ä½äº `data/server_bots/aistream.yaml`ï¼š

**å…³é”®é…ç½®é¡¹**ï¼š
- `llm.Provider` - LLMæä¾›å•†ï¼ˆ`gptgod`/`volcengine`/`xiaomimimo`/`openai`/`openai_compat`/`gemini`/`anthropic`/`azure_openai`ï¼‰
- `subserver.host` - å­æœåŠ¡ç«¯åœ°å€ï¼ˆé»˜è®¤ `127.0.0.1`ï¼‰
- `subserver.port` - å­æœåŠ¡ç«¯ç«¯å£ï¼ˆé»˜è®¤ `8000`ï¼‰
- `subserver.timeout` - è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ `30000`ï¼‰

**LLMæä¾›å•†é…ç½®**ï¼š
- é…ç½®æ–‡ä»¶ï¼š`data/server_bots/{port}/{providerName}_llm.yaml`
- é…ç½®åˆå¹¶ä¼˜å…ˆçº§ï¼š`apiConfig` > `providerConfig` > `this.config` > é»˜è®¤å€¼
- æ”¯æŒåŠ¨æ€æ‰©å±•ï¼Œæ— éœ€ä¿®æ”¹åŸºç±»ä»£ç 
- `enableTools`ï¼šæ§åˆ¶æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œç”±å„æä¾›å•†é…ç½®å†³å®š
- `proxy`ï¼šå¯é€‰ä»£ç†é…ç½®ï¼Œä»…å½±å“ä¸»æœåŠ¡ç«¯ä» **æœ¬æœºåˆ°å„å‚å•† LLM æ¥å£** çš„ HTTP è¯·æ±‚ï¼Œä¸ä¼šä¿®æ”¹ç³»ç»Ÿå…¨å±€ä»£ç†  
  - å¯¹è±¡å½¢å¼ï¼š
    - `proxy.enabled: true|false`ï¼šæ˜¯å¦å¯ç”¨ä»£ç†ï¼ˆé»˜è®¤ä¸º `false`ï¼Œæœªé…ç½®è§†ä¸ºä¸å¯ç”¨ï¼‰
    - `proxy.url: "http://user:pass@host:port"`ï¼šæ ‡å‡† HTTP/HTTPS/SOCKS5 ä»£ç†åœ°å€
  - ç®€å†™å½¢å¼ï¼š`proxy: "http://user:pass@host:port"`ï¼ˆç­‰ä»·äº `enabled: true` ä¸”ä½¿ç”¨è¯¥åœ°å€ï¼‰
  - ä»…æ”¯æŒæ ‡å‡†ä»£ç†åè®®ï¼›**vmess/vless ç­‰è®¢é˜…éœ€ç”± Clash / sing-box ç­‰ç‹¬ç«‹å®¢æˆ·ç«¯è½¬æ¢ä¸º HTTP ä»£ç†åå†ç”± `proxy.url` æŒ‡å‘**

**å…³äº modelï¼ˆå¤–éƒ¨è°ƒç”¨çº¦å®šï¼‰**ï¼š
- å¯¹å¤– v3 å…¥å£ `POST /api/v3/chat/completions`ï¼šå¤–éƒ¨è°ƒç”¨åªéœ€è¦æŠŠ `model` å¡«æˆ **providerï¼ˆè¿è¥å•†ï¼‰**ï¼ˆå¦‚ `openai` / `openai_compat` / `gemini` ç­‰ï¼‰ï¼Œ**ä¸éœ€è¦**å†å¡«å†™çœŸå®æ¨¡å‹åã€‚
- çœŸå®æ¨¡å‹åç”± `{provider}_llm.yaml` ä¸­çš„é»˜è®¤ `model`/`chatModel` å†³å®šï¼›ä½ ä¹Ÿå¯ä»¥é€šè¿‡å·¥ä½œæµ/å†…éƒ¨é…ç½®è¦†ç›–ï¼Œä½†å¤–éƒ¨è°ƒç”¨ä¸å¼ºåˆ¶è¦æ±‚ã€‚

**Embeddingé…ç½®**ï¼š
- ç»Ÿä¸€ä½¿ç”¨å­æœåŠ¡ç«¯å‘é‡æœåŠ¡ï¼ˆ`/api/vector/*`ï¼‰
- å·¥ä½œæµæ„é€ å‡½æ•°åªéœ€è®¾ç½® `embedding: { enabled: true, maxContexts: 5 }`
- å‘é‡æœåŠ¡é…ç½®ä½äºå­æœåŠ¡ç«¯é…ç½®æ–‡ä»¶ï¼ˆ`data/subserver/config.yaml`ï¼‰
- `maxContexts` ä¸ºå·¥ä½œæµçº§åˆ«é…ç½®ï¼Œæ§åˆ¶æ£€ç´¢ä¸Šä¸‹æ–‡æ¡æ•°

---

## æ ¸å¿ƒæ–¹æ³•

### `async init()`

åˆå§‹åŒ–å·¥ä½œæµï¼ˆä»…æ‰§è¡Œä¸€æ¬¡ï¼‰ï¼Œç”± `StreamLoader` åœ¨åŠ è½½æ—¶è‡ªåŠ¨è°ƒç”¨ã€‚

**åˆå§‹åŒ–å†…å®¹**ï¼š
- è‹¥å°šæœªå­˜åœ¨ï¼Œåˆ™åˆå§‹åŒ– MCP å·¥å…·æ˜ å°„ `this.mcpTools = new Map()`
- å­ç±»å¯é‡å†™æ­¤æ–¹æ³•è¿›è¡Œè‡ªå®šä¹‰åˆå§‹åŒ–ï¼ˆä¾‹å¦‚æ³¨å†Œ MCP å·¥å…·ï¼‰

### `buildSystemPrompt(context)` / `buildChatContext(e, question)`

æŠ½è±¡æ–¹æ³•ï¼ˆå¯é€‰å®ç°ï¼‰ï¼š
- `buildSystemPrompt` - æ„å»ºç³»ç»Ÿçº§æç¤ºè¯ï¼ˆè§’è‰²è®¾å®šã€å›å¤é£æ ¼ç­‰ï¼‰
- `buildChatContext` - å°†äº‹ä»¶ä¸ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸º `messages` æ•°ç»„

> è‹¥å­ç±»æœªå®ç°ï¼ŒåŸºç±»ä¼šæä¾›é»˜è®¤å®ç°ï¼ˆè¿”å›ç©ºå­—ç¬¦ä¸²/ç©ºæ•°ç»„ï¼‰

---

## Embedding ä¸ä¸Šä¸‹æ–‡å¢å¼º

**é‡è¦è¯´æ˜**ï¼š
- **å‘é‡æœåŠ¡ç»Ÿä¸€ç”±å­æœåŠ¡ç«¯æä¾›**ï¼Œä¸»æœåŠ¡ç«¯åªéœ€é…ç½®å­æœåŠ¡ç«¯è¿æ¥ä¿¡æ¯ï¼ˆ`subserver.host`ã€`subserver.port`ã€`subserver.timeout`ï¼‰
- å‘é‡æœåŠ¡é…ç½®ï¼ˆæ¨¡å‹ã€ç»´åº¦ç­‰ï¼‰ä½äºå­æœåŠ¡ç«¯é…ç½®æ–‡ä»¶ï¼ˆ`data/subserver/config.yaml`ï¼‰
- å·¥ä½œæµåªéœ€è®¾ç½® `embedding: { enabled: true, maxContexts: 5 }` å³å¯å¯ç”¨
- `maxContexts` ä¸ºå·¥ä½œæµçº§åˆ«é…ç½®ï¼Œæ§åˆ¶æ£€ç´¢ä¸Šä¸‹æ–‡æ¡æ•°ï¼Œä¸æ˜¯å‘é‡æœåŠ¡é…ç½®

**æ ¸å¿ƒæ–¹æ³•**ï¼š

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `generateEmbedding(text)` | è°ƒç”¨å­æœåŠ¡ç«¯ `/api/vector/embed` ç”Ÿæˆæ–‡æœ¬å‘é‡ |
| `storeMessageWithEmbedding(groupId, message)` | å­˜å‚¨æ¶ˆæ¯åˆ°å‘é‡æ•°æ®åº“å’ŒRedisï¼ˆkey: `ai:memory:${name}:${groupId}`ï¼‰ |
| `retrieveRelevantContexts(groupId, query)` | æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨MemoryManagerï¼Œå†è°ƒç”¨å­æœåŠ¡ç«¯å‘é‡æ£€ç´¢ï¼‰ |
| `buildEnhancedContext(e, question, baseMessages)` | æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡ï¼ˆå®Œæ•´RAGæµç¨‹ï¼šå†å²å¯¹è¯ + çŸ¥è¯†åº“ï¼‰ |

**å‘é‡æœåŠ¡æ¥å£**ï¼ˆå­æœåŠ¡ç«¯ï¼‰ï¼š
- `POST /api/vector/embed` - æ–‡æœ¬å‘é‡åŒ–ï¼ˆç”±å­æœåŠ¡ç«¯æä¾›ï¼‰
- `POST /api/vector/search` - å‘é‡æ£€ç´¢ï¼ˆç”±å­æœåŠ¡ç«¯æä¾›ï¼‰
- `POST /api/vector/upsert` - å‘é‡å…¥åº“ï¼ˆç”±å­æœåŠ¡ç«¯æä¾›ï¼‰

**å­æœåŠ¡ç«¯é…ç½®**ï¼š
- é…ç½®æ–‡ä»¶ï¼š`data/subserver/config.yaml`
- å‘é‡æ¨¡å‹ã€ç»´åº¦ç­‰é…ç½®åœ¨å­æœåŠ¡ç«¯é…ç½®æ–‡ä»¶ä¸­è®¾ç½®

---

## å‡½æ•°è°ƒç”¨ä¸ MCP å·¥å…·

AIStream **ä¸å†è§£æ/æ‰§è¡Œä»»ä½•â€œæ–‡æœ¬å‡½æ•°è°ƒç”¨ / ReActâ€**ï¼Œæ‰€æœ‰å·¥å…·è°ƒç”¨å‡é€šè¿‡ **LLM å·¥å‚çš„ tool calling + MCP åè®®** å®Œæˆï¼š

- **tool calls å¤šè½®äº¤äº’**ï¼šç”± LLMFactory åŠå„æä¾›å•†å®¢æˆ·ç«¯å†…éƒ¨å¤„ç† `tool_calls` å¾ªç¯ï¼Œæœ€ç»ˆè¿”å›æ•´ç†å¥½çš„ `assistant.content` æ–‡æœ¬ç»™ AIStreamã€‚
- **MCP å·¥å…·æ³¨å†Œ**ï¼šAIStream é€šè¿‡ `registerMCPTool(name, options)` å°†å·¥å…·æ³¨å†Œåˆ° `this.mcpTools`ï¼Œä¾› MCP æœåŠ¡å™¨å‘ç°å’Œè°ƒç”¨ã€‚

### `registerMCPTool(name, options)`

æ³¨å†Œ MCP å·¥å…·ï¼ˆä¾› MCP åè®®è°ƒç”¨çš„æ ‡å‡†å·¥å…·ï¼‰ã€‚

**å‚æ•°**ï¼š
- `name` - å·¥å…·åç§°
- `options.handler` - å·¥å…·å¤„ç†å‡½æ•° `async (args, context) => {...}`ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ
- `options.description` - å·¥å…·æè¿°
- `options.inputSchema` - JSON Schema æ ¼å¼çš„è¾“å…¥å‚æ•°å®šä¹‰
- `options.enabled` - æ˜¯å¦å¯ç”¨ï¼ˆå¯è¢« `functionToggles` è¦†ç›–ï¼‰

> å·¥å…·è¿”å›å€¼æ¨èä½¿ç”¨ `successResponse(data)` / `errorResponse(code, message)` è¿›è¡ŒåŒ…è£…ï¼š
> - `successResponse(data)` â†’ `{ success: true, data: { ...data, timestamp } }`
> - `errorResponse(code, message)` â†’ `{ success: false, error: { code, message } }`

---

## LLM è°ƒç”¨

> **æç¤º**ï¼šå…³äº LLM å·¥å‚çš„è¯¦ç»†è¯´æ˜ã€æ”¯æŒçš„æä¾›å•†åˆ—è¡¨ã€å¦‚ä½•æ‰©å±•æ–°æä¾›å•†ç­‰ï¼Œè¯·å‚è€ƒ **[å·¥å‚ç³»ç»Ÿæ–‡æ¡£](factory.md)**ã€‚

```mermaid
sequenceDiagram
    participant Plugin as ğŸ”Œ æ’ä»¶
    participant Stream as ğŸŒŠ AIStream
    participant Subserver as ğŸ Pythonå­æœåŠ¡ç«¯
    participant LLM as ğŸ¤– LLMå·¥å‚
    participant Vector as ğŸ“Š å‘é‡æœåŠ¡
    
    Note over Plugin,Vector: ğŸ”„ LLM è°ƒç”¨æµç¨‹
    
    Plugin->>Stream: ğŸ“ process(e, question, options)<br/>è°ƒç”¨å·¥ä½œæµ
    Stream->>Stream: ğŸ“ buildChatContext(e, question)<br/>æ„å»ºåŸºç¡€æ¶ˆæ¯
    Stream->>Stream: ğŸ” buildEnhancedContext(e, question)<br/>RAGå¢å¼ºä¸Šä¸‹æ–‡
    Stream->>Subserver: ğŸŒ POST /api/langchain/chat<br/>LangChainç¼–æ’
    
    alt ğŸ å­æœåŠ¡ç«¯å¯ç”¨
        Subserver->>LLM: ğŸ“¡ POST /api/v3/chat/completions<br/>è°ƒç”¨LLMå·¥å‚
        LLM-->>Subserver: âœ… è¿”å›å“åº”<br/>AIå›å¤æ–‡æœ¬
        Subserver-->>Stream: âœ… è¿”å›ç»“æœ<br/>Agentå¤„ç†ç»“æœ
    else âš™ï¸ å­æœåŠ¡ç«¯ä¸å¯ç”¨
        Stream->>LLM: ğŸ“¡ ç›´æ¥è°ƒç”¨LLMå·¥å‚<br/>LLMFactory.createClient()
        LLM-->>Stream: âœ… è¿”å›å“åº”<br/>AIå›å¤æ–‡æœ¬
    end
    
    Stream->>Vector: ğŸ’¾ POST /api/vector/upsert<br/>å­˜å‚¨æ¶ˆæ¯å‘é‡
    Vector-->>Stream: âœ… å­˜å‚¨æˆåŠŸ
    Stream-->>Plugin: âœ… è¿”å›ç»“æœ<br/>æœ€ç»ˆå“åº”
    
    Note over Plugin: âœ¨ è°ƒç”¨å®Œæˆ
```

**æ ¸å¿ƒæ–¹æ³•**ï¼š

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `callAI(messages, apiConfig)` | éæµå¼è°ƒç”¨AIæ¥å£ï¼ˆä¼˜å…ˆå­æœåŠ¡ç«¯LangChainï¼Œå¤±è´¥æ—¶å›é€€åˆ°LLMå·¥å‚ï¼‰ |
| `callAIStream(messages, apiConfig, onDelta, options)` | æµå¼è°ƒç”¨AIæ¥å£ï¼Œé€šè¿‡ `onDelta` å›è°ƒè¿”å›å¢é‡æ–‡æœ¬ |
| `execute(e, question, config)` | æ‰§è¡Œï¼šæ„å»ºä¸Šä¸‹æ–‡ â†’ è°ƒç”¨LLMï¼ˆå« MCP tool callingï¼‰â†’ å­˜å‚¨è®°å¿† |
| `process(e, question, options)` | å·¥ä½œæµå¤„ç†å…¥å£ï¼ˆå•æ¬¡å¯¹è¯ + MCP å·¥å…·è°ƒç”¨ï¼›å¤æ‚å¤šæ­¥ç¼–æ’åœ¨ Python å­æœåŠ¡ç«¯ï¼‰ |

**process æ–¹æ³•å‚æ•°**ï¼š
- `mergeStreams` - è¦åˆå¹¶çš„å·¥ä½œæµåç§°åˆ—è¡¨
- `enableMemory` - æ˜¯å¦å¯ç”¨è®°å¿†ç³»ç»Ÿï¼ˆé»˜è®¤ `false`ï¼‰
- `enableDatabase` - æ˜¯å¦å¯ç”¨çŸ¥è¯†åº“ç³»ç»Ÿï¼ˆé»˜è®¤ `false`ï¼‰
- `apiConfig` - LLMé…ç½®ï¼ˆå¯é€‰ï¼Œä¼šä¸ `this.config` åˆå¹¶ï¼‰

**è°ƒç”¨æµç¨‹**ï¼š
1. `buildChatContext` - æ„å»ºåŸºç¡€æ¶ˆæ¯æ•°ç»„
2. `buildEnhancedContext` - RAGæµç¨‹ï¼šæ£€ç´¢å†å²å¯¹è¯å’ŒçŸ¥è¯†åº“
3. `callAI` - è°ƒç”¨LLMï¼ˆä¼˜å…ˆå­æœåŠ¡ç«¯LangChainï¼Œå¤±è´¥æ—¶å›é€€åˆ°LLMå·¥å‚ï¼‰
4. `storeMessageWithEmbedding` - å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿï¼ˆé€šè¿‡å­æœåŠ¡ç«¯å‘é‡æœåŠ¡ï¼‰
5. è‡ªåŠ¨å‘é€å›å¤ï¼ˆæ’ä»¶ä¸éœ€è¦å†æ¬¡è°ƒç”¨ `reply()`ï¼‰

**å­æœåŠ¡ç«¯é›†æˆè¯¦ç»†æµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant AIStream as ğŸŒŠ AIStream
    participant Subserver as ğŸ Pythonå­æœåŠ¡ç«¯
    participant LangChain as ğŸŒ LangChain Agent
    participant MainServer as âš™ï¸ ä¸»æœåŠ¡ç«¯v3
    participant MCP as ğŸ”§ MCPæœåŠ¡å™¨
    participant Vector as ğŸ“Š å‘é‡æœåŠ¡
    
    Note over AIStream,Vector: ğŸ”„ LLMè°ƒç”¨æµç¨‹ï¼ˆå­æœåŠ¡ç«¯ï¼‰
    
    AIStream->>Subserver: ğŸŒ POST /api/langchain/chat<br/>è¯·æ±‚Agentå¤„ç†
    Subserver->>LangChain: ğŸ¤– åˆ›å»ºAgentå¹¶å¤„ç†æ¶ˆæ¯<br/>LangChain Agent
    LangChain->>MainServer: ğŸ“¡ POST /api/v3/chat/completions<br/>è°ƒç”¨LLMå·¥å‚
    
    alt ğŸ”§ éœ€è¦å·¥å…·è°ƒç”¨
        MainServer->>MCP: ğŸ”§ æ‰§è¡ŒMCPå·¥å…·<br/>tools/call
        MCP-->>MainServer: âœ… å·¥å…·ç»“æœ<br/>JSONæ ¼å¼
        MainServer-->>LangChain: ğŸ“¤ åŒ…å«å·¥å…·ç»“æœçš„å“åº”<br/>LLMå“åº”+å·¥å…·ç»“æœ
        LangChain->>MainServer: ğŸ“¡ å†æ¬¡è°ƒç”¨ï¼ˆå¤šè½®å¯¹è¯ï¼‰<br/>ç»§ç»­Agentæµç¨‹
    end
    
    MainServer-->>LangChain: âœ… æœ€ç»ˆLLMå“åº”<br/>AIå›å¤æ–‡æœ¬
    LangChain-->>Subserver: âœ… Agentå¤„ç†ç»“æœ<br/>æœ€ç»ˆå“åº”
    Subserver-->>AIStream: âœ… è¿”å›å“åº”<br/>å·¥ä½œæµç»“æœ
    
    Note over AIStream,Vector: ğŸ“Š å‘é‡æœåŠ¡æµç¨‹
    
    AIStream->>Subserver: ğŸ” POST /api/vector/search<br/>æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    Subserver->>Vector: ğŸ“Š ChromaDBæ£€ç´¢<br/>å‘é‡ç›¸ä¼¼åº¦æœç´¢
    Vector-->>Subserver: âœ… æ£€ç´¢ç»“æœ<br/>ç›¸å…³ä¸Šä¸‹æ–‡åˆ—è¡¨
    Subserver-->>AIStream: ğŸ“‹ è¿”å›ä¸Šä¸‹æ–‡<br/>å¢å¼ºæ¶ˆæ¯
    
    AIStream->>Subserver: ğŸ’¾ POST /api/vector/upsert<br/>å­˜å‚¨æ¶ˆæ¯å‘é‡
    Subserver->>Vector: ğŸ“Š å­˜å‚¨å‘é‡<br/>ChromaDB upsert
    Vector-->>Subserver: âœ… å­˜å‚¨æˆåŠŸ
    Subserver-->>AIStream: âœ… ç¡®è®¤<br/>å­˜å‚¨å®Œæˆ
    
    Note over AIStream: âœ¨ æµç¨‹å®Œæˆ
```

---

## å®Œæ•´APIå‚è€ƒ

### æ ¸å¿ƒæ–¹æ³•è¯¦è§£

#### `async process(e, question, options)`

å·¥ä½œæµå¤„ç†å…¥å£ï¼Œæ”¯æŒå·¥ä½œæµåˆå¹¶å’Œä¸Šä¸‹æ–‡å¢å¼ºã€‚

**å‚æ•°**ï¼š
- `e` - äº‹ä»¶å¯¹è±¡ï¼ˆQQ/IM/Chatbot ç­‰æ¶ˆæ¯äº‹ä»¶ï¼‰
- `question` - ç”¨æˆ·é—®é¢˜ï¼ˆå­—ç¬¦ä¸²æˆ–å¯¹è±¡ï¼‰
- `options` - é€‰é¡¹å¯¹è±¡
  - `mergeStreams` - è¦åˆå¹¶çš„å·¥ä½œæµåç§°æ•°ç»„
  - `enableMemory` - æ˜¯å¦å¯ç”¨è®°å¿†ç³»ç»Ÿï¼ˆè‡ªåŠ¨åˆå¹¶ `memory` å·¥ä½œæµï¼‰
  - `enableDatabase` - æ˜¯å¦å¯ç”¨çŸ¥è¯†åº“ç³»ç»Ÿï¼ˆè‡ªåŠ¨åˆå¹¶ `database` å·¥ä½œæµï¼‰
  - `apiConfig` - LLMé…ç½®è¦†ç›–ï¼ˆprovider, model, temperatureç­‰ï¼‰

**è¿”å›**ï¼š`Promise<string|null>` - AIå›å¤æ–‡æœ¬

**ç¤ºä¾‹**ï¼š
```javascript
// åŸºç¡€è°ƒç”¨
await stream.process(e, e.msg);

// å¯ç”¨è®°å¿†å’ŒçŸ¥è¯†åº“
await stream.process(e, e.msg, {
  enableMemory: true,
  enableDatabase: true
});

// åˆå¹¶å¤šä¸ªå·¥ä½œæµ
await stream.process(e, e.msg, {
  mergeStreams: ['tools', 'memory']
});

// è‡ªå®šä¹‰LLMé…ç½®
await stream.process(e, e.msg, {
  apiConfig: {
    provider: 'gptgod',
    model: 'gpt-4',
    temperature: 0.7
  }
});
```

#### `async callAI(messages, apiConfig)`

éæµå¼è°ƒç”¨AIæ¥å£ï¼Œæ”¯æŒé‡è¯•å’Œé”™è¯¯å¤„ç†ã€‚

**å‚æ•°**ï¼š
- `messages` - æ¶ˆæ¯æ•°ç»„ï¼ˆOpenAIæ ¼å¼ï¼‰
- `apiConfig` - APIé…ç½®ï¼ˆå¯é€‰ï¼‰

**è¿”å›**ï¼š`Promise<string>` - AIå›å¤æ–‡æœ¬

**ç‰¹ç‚¹**ï¼š
- ä¼˜å…ˆä½¿ç”¨å­æœåŠ¡ç«¯ï¼ˆLangChainï¼‰
- å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°LLMå·¥å‚
- æ”¯æŒé‡è¯•æœºåˆ¶ï¼ˆå¯é…ç½®ï¼‰
- è‡ªåŠ¨è®°å½•Tokenä½¿ç”¨å’Œæˆæœ¬

#### `async callAIStream(messages, apiConfig, onDelta, options)`

æµå¼è°ƒç”¨AIæ¥å£ï¼Œå®æ—¶è¿”å›å¢é‡æ–‡æœ¬ã€‚

**å‚æ•°**ï¼š
- `messages` - æ¶ˆæ¯æ•°ç»„
- `apiConfig` - APIé…ç½®
- `onDelta` - å¢é‡å›è°ƒå‡½æ•° `(delta: string) => void`
- `options` - é€‰é¡¹ï¼ˆå¯é€‰ï¼‰

**è¿”å›**ï¼š`Promise<string>` - å®Œæ•´å›å¤æ–‡æœ¬

**ç¤ºä¾‹**ï¼š
```javascript
let fullText = '';
await stream.callAIStream(messages, {}, (delta) => {
  fullText += delta;
  // å®æ—¶å‘é€å¢é‡æ–‡æœ¬
  e.reply(delta);
});
```

#### `async buildEnhancedContext(e, question, baseMessages)`

æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡ï¼ˆRAGæµç¨‹ï¼‰ã€‚

**æµç¨‹**ï¼š
1. æå–æŸ¥è¯¢æ–‡æœ¬
2. æ£€ç´¢å†å²å¯¹è¯ï¼ˆ`retrieveRelevantContexts`ï¼‰
3. æ£€ç´¢çŸ¥è¯†åº“ï¼ˆ`retrieveKnowledgeContexts`ï¼‰
4. ä¼˜åŒ–å’Œå‹ç¼©ä¸Šä¸‹æ–‡
5. åˆå¹¶åˆ°æ¶ˆæ¯æ•°ç»„

**è¿”å›**ï¼š`Promise<Array>` - å¢å¼ºåçš„æ¶ˆæ¯æ•°ç»„

### ä¸Šä¸‹æ–‡æ£€ç´¢æ–¹æ³•

#### `async retrieveRelevantContexts(groupId, query)`

æ£€ç´¢ç›¸å…³å†å²å¯¹è¯ã€‚

**å‚æ•°**ï¼š
- `groupId` - ç¾¤ç»„IDæˆ–ç”¨æˆ·ID
- `query` - æŸ¥è¯¢æ–‡æœ¬

**è¿”å›**ï¼š`Promise<Array>` - ä¸Šä¸‹æ–‡æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- `message` - æ¶ˆæ¯å†…å®¹
- `similarity` - ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
- `time` - æ—¶é—´æˆ³
- `userId` - ç”¨æˆ·ID
- `nickname` - æ˜µç§°

#### `async retrieveKnowledgeContexts(query)`

æ£€ç´¢çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä»åˆå¹¶çš„å·¥ä½œæµä¸­æŸ¥æ‰¾ï¼‰ã€‚

**å‚æ•°**ï¼š
- `query` - æŸ¥è¯¢æ–‡æœ¬

**è¿”å›**ï¼š`Promise<Array>` - çŸ¥è¯†åº“ä¸Šä¸‹æ–‡æ•°ç»„

### å·¥ä½œæµåˆå¹¶

#### `merge(stream, options)`

åˆå¹¶å…¶ä»–å·¥ä½œæµçš„åŠŸèƒ½ã€‚

**å‚æ•°**ï¼š
- `stream` - è¦åˆå¹¶çš„å·¥ä½œæµå®ä¾‹
- `options` - é€‰é¡¹
  - `overwrite` - æ˜¯å¦è¦†ç›–åŒåå‡½æ•°ï¼ˆé»˜è®¤ `false`ï¼‰
  - `prefix` - å‡½æ•°åå‰ç¼€ï¼ˆé»˜è®¤ `''`ï¼‰

**è¿”å›**ï¼š`Object` - `{ mergedCount, skippedCount }`

**ç¤ºä¾‹**ï¼š
```javascript
const toolsStream = StreamLoader.getStream('tools');
this.merge(toolsStream, { prefix: 'tools.' });
```

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€å·¥ä½œæµå®ç°

```javascript
import AIStream from '#infrastructure/aistream/aistream.js';

export default class MyStream extends AIStream {
  constructor() {
    super({
      name: 'my-stream',
      description: 'æˆ‘çš„è‡ªå®šä¹‰å·¥ä½œæµ',
      version: '1.0.0',
      priority: 50,
      config: {
        temperature: 0.8,
        maxTokens: 2000
      },
      embedding: { enabled: true }
    });
  }

  async init() {
    await super.init();
    // åœ¨æ­¤æ³¨å†Œ MCP å·¥å…·ç­‰åˆå§‹åŒ–é€»è¾‘
    this.registerMCPTool('get_info', {
      description: 'è·å–ä¿¡æ¯',
      inputSchema: {
        type: 'object',
        properties: {
          key: { type: 'string' }
        },
        required: ['key']
      },
      handler: async (args, context) => {
        // è¿”å›ç»Ÿä¸€ç»“æ„
        return this.successResponse({ value: `you asked for ${args.key}` });
      }
    });
  }

  buildSystemPrompt(context) {
    return 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹...';
  }

  async buildChatContext(e, question) {
    const messages = [];
    messages.push({
      role: 'system',
      content: this.buildSystemPrompt({ e, question })
    });
    messages.push({
      role: 'user',
      content: typeof question === 'string' ? question : question?.text || ''
    });
    return messages;
  }
}
```

### æ’ä»¶ä¸­è°ƒç”¨å·¥ä½œæµ

```javascript
// åŸºç¡€è°ƒç”¨
const stream = this.getStream('chat');
await stream.process(e, e.msg);

// å¯ç”¨è®°å¿†å’ŒçŸ¥è¯†åº“
await stream.process(e, e.msg, {
  enableMemory: true,
  enableDatabase: true
});

// åˆå¹¶å¤šä¸ªå·¥ä½œæµ
await stream.process(e, e.msg, {
  mergeStreams: ['tools', 'memory']
});

// è‡ªå®šä¹‰LLMé…ç½®
await stream.process(e, e.msg, {
  apiConfig: {
    provider: 'gptgod',
    model: 'gpt-4',
    temperature: 0.7
  }
});

// æµå¼è°ƒç”¨ï¼ˆéœ€è¦æ‰‹åŠ¨å‘é€å›å¤ï¼‰
let fullText = '';
await stream.callAIStream(messages, {}, (delta) => {
  fullText += delta;
  e.reply(delta);
});
```

### å·¥ä½œæµåˆå¹¶ç¤ºä¾‹

```javascript
// åœ¨desktopå·¥ä½œæµä¸­åˆå¹¶toolså·¥ä½œæµ
async init() {
  await super.init();

  const toolsStream = StreamLoader.getStream('tools');
  if (toolsStream) {
    this.merge(toolsStream);
  }
}
```

---

## å­æœåŠ¡ç«¯é›†æˆ

AIStreamç³»ç»Ÿä¸Pythonå­æœåŠ¡ç«¯ç´§å¯†é›†æˆï¼Œå®ç°LLMè°ƒç”¨å’Œå‘é‡æœåŠ¡çš„ç»Ÿä¸€ç®¡ç†ã€‚

### æ¶æ„è®¾è®¡

```
ä¸»æœåŠ¡ç«¯ (Node.js)                    Pythonå­æœåŠ¡ç«¯ (FastAPI)
â”œâ”€ AIStreamåŸºç±»          â”€â”€â”€â”€â”€â”€HTTPâ”€â”€â”€â”€â”€â”€>  â”œâ”€ LangChainæœåŠ¡
â”œâ”€ LLMå·¥å‚                                  â”‚  â””â”€ Agentç¼–æ’
â”œâ”€ MCPæœåŠ¡å™¨                                â”‚  â””â”€ å·¥å…·è°ƒç”¨
â””â”€ æ’ä»¶/å·¥ä½œæµ                             â””â”€ å‘é‡æœåŠ¡
                                              â”œâ”€ å‘é‡åŒ– (embed)
                                              â”œâ”€ å‘é‡æ£€ç´¢ (search)
                                              â””â”€ å‘é‡å…¥åº“ (upsert)
```

**æ ¸å¿ƒåŸåˆ™**ï¼š
- **ä¸»æœåŠ¡ç«¯**ï¼šç»Ÿä¸€LLM Providerå…¥å£ã€MCPå·¥å…·æ‰§è¡Œã€å·¥ä½œæµç®¡ç†
- **å­æœåŠ¡ç«¯**ï¼šLangChainç”Ÿæ€ã€å‘é‡æœåŠ¡ã€Python AIèƒ½åŠ›

### å‘é‡æœåŠ¡æ¥å£

AIStreamé€šè¿‡å­æœåŠ¡ç«¯æä¾›å‘é‡åŒ–æœåŠ¡ï¼ˆç»Ÿä¸€é€šè¿‡ `Bot.callSubserver` è°ƒç”¨ï¼‰ï¼š

- **POST /api/vector/embed** - æ–‡æœ¬å‘é‡åŒ–
  ```json
  {
    "texts": ["æ–‡æœ¬1", "æ–‡æœ¬2"]
  }
  ```
  è¿”å›ï¼š`{ embeddings: [{ text, embedding }] }`

- **POST /api/vector/search** - å‘é‡æ£€ç´¢
  ```json
  {
    "query": "æŸ¥è¯¢æ–‡æœ¬",
    "collection": "memory_group123",
    "top_k": 5
  }
  ```
  è¿”å›ï¼š`{ results: [{ text, score, metadata }] }`

- **POST /api/vector/upsert** - å‘é‡å…¥åº“
  ```json
  {
    "collection": "memory_group123",
    "documents": [{
      "text": "æ–‡æœ¬å†…å®¹",
      "metadata": {}
    }]
  }
  ```

### LLMè°ƒç”¨æ¥å£

- **POST /api/langchain/chat** - LLMå¯¹è¯ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
  ```json
  {
    "messages": [...],
    "model": "gptgod",
    "enableTools": false
    "temperature": 0.8,
    "max_tokens": 2000,
    "stream": false,
    "enableTools": true
  }
  ```

**å‚æ•°åˆ«åå…¼å®¹ï¼ˆåŒä¹‰å­—æ®µï¼‰**ï¼š
- `apiKey` â†” `api_key`
- `max_tokens` â†” `maxTokens` â†” `max_completion_tokens`
- `top_p` â†” `topP`
- `presence_penalty` â†” `presencePenalty`
- `frequency_penalty` â†” `frequencyPenalty`
- `tool_choice` â†” `toolChoice`
- `parallel_tool_calls` â†” `parallelToolCalls`
- `extraBody`ï¼šå¯é€‰æ‰©å±•å­—æ®µï¼ˆå¯¹è±¡æˆ– JSON å­—ç¬¦ä¸²ï¼‰
  
  **è°ƒç”¨æµç¨‹**ï¼š
  1. AIStreamè°ƒç”¨å­æœåŠ¡ç«¯ `/api/langchain/chat`
  2. å­æœåŠ¡ç«¯é€šè¿‡LangChain Agentå¤„ç†æ¶ˆæ¯
  3. å­æœåŠ¡ç«¯è°ƒç”¨ä¸»æœåŠ¡ç«¯ `/api/v3/chat/completions` è·å–LLMå“åº”
  4. å¦‚éœ€å·¥å…·è°ƒç”¨ï¼Œä¸»æœåŠ¡ç«¯æ‰§è¡ŒMCPå·¥å…·å¹¶è¿”å›ç»“æœ
  5. å­æœåŠ¡ç«¯è¿”å›æœ€ç»ˆå“åº”ç»™AIStream

  **å›é€€æœºåˆ¶**ï¼šå¦‚æœå­æœåŠ¡ç«¯ä¸å¯ç”¨ï¼ŒAIStreamè‡ªåŠ¨å›é€€åˆ°ç›´æ¥è°ƒç”¨LLMå·¥å‚ã€‚

### é”™è¯¯å¤„ç†

- å­æœåŠ¡ç«¯è°ƒç”¨å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°LLMå·¥å‚
- å‘é‡æœåŠ¡è°ƒç”¨å¤±è´¥æ—¶ï¼Œè®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­æµç¨‹
- æ”¯æŒé‡è¯•æœºåˆ¶ï¼ˆå¯é…ç½®ï¼‰

---

## é”™è¯¯å¤„ç†ä¸é‡è¯•

### é‡è¯•é…ç½®

åœ¨ `aistream.yaml` ä¸­é…ç½®ï¼š

```yaml
llm:
  retry:
    enabled: true
    maxAttempts: 3
    delay: 2000
    maxDelay: 10000
    backoffMultiplier: 2
    retryOn: ["timeout", "network", "5xx", "rate_limit"]
```

### é”™è¯¯åˆ†ç±»

ç³»ç»Ÿè‡ªåŠ¨åˆ†ç±»é”™è¯¯ç±»å‹ï¼š
- `timeout` - è¶…æ—¶é”™è¯¯
- `network` - ç½‘ç»œé”™è¯¯
- `5xx` - æœåŠ¡å™¨é”™è¯¯
- `rate_limit` - é™æµé”™è¯¯
- `auth` - è®¤è¯é”™è¯¯ï¼ˆä¸é‡è¯•ï¼‰

---

## æ€§èƒ½ä¼˜åŒ–

### ä¸Šä¸‹æ–‡ä¼˜åŒ–

- **è‡ªåŠ¨å»é‡**ï¼š`deduplicateContexts()` å»é™¤é‡å¤ä¸Šä¸‹æ–‡
- **æ™ºèƒ½å‹ç¼©**ï¼š`optimizeContexts()` æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶å‹ç¼©
- **Tokenä¼°ç®—**ï¼š`estimateTokens()` ä¼°ç®—æ–‡æœ¬tokenæ•°é‡

### ç¼“å­˜æœºåˆ¶

- Embeddingç»“æœç¼“å­˜ï¼ˆé€šè¿‡å­æœåŠ¡ç«¯ï¼‰
- ä¸Šä¸‹æ–‡æ£€ç´¢ç»“æœç¼“å­˜
- å·¥ä½œæµå®ä¾‹ç¼“å­˜ï¼ˆStreamLoaderï¼‰

---

## ç›‘æ§ä¸è¿½è¸ª

### MonitorServiceé›†æˆ

å·¥ä½œæµæ‰§è¡Œè‡ªåŠ¨è®°å½•ï¼š
- æ‰§è¡Œè¿½è¸ªï¼ˆtraceIdï¼‰
- Tokenä½¿ç”¨ç»Ÿè®¡
- æˆæœ¬ç»Ÿè®¡
- é”™è¯¯æ—¥å¿—

**ç¤ºä¾‹**ï¼š
```javascript
const traceId = MonitorService.startTrace(this.name, {
  agentId: e?.user_id,
  workflow: this.name
});

// ... æ‰§è¡Œé€»è¾‘ ...

MonitorService.endTrace(traceId, { success: true });
```

---

## ç›¸å…³æ–‡æ¡£

- **[system-Core ç‰¹æ€§](system-core.md)** - system-Core å†…ç½®æ¨¡å—å®Œæ•´è¯´æ˜ï¼ŒåŒ…å«7ä¸ªå·¥ä½œæµçš„å®é™…å®ç°ï¼ˆchatã€desktopã€toolsã€memoryã€databaseã€deviceã€doc-indexerï¼‰ â­
- **[æ¡†æ¶å¯æ‰©å±•æ€§æŒ‡å—](æ¡†æ¶å¯æ‰©å±•æ€§æŒ‡å—.md)** - æ‰©å±•å¼€å‘å®Œæ•´æŒ‡å—
- **[å·¥å‚ç³»ç»Ÿ](factory.md)** - LLM/Vision/ASR/TTS å·¥å‚ç³»ç»Ÿï¼Œç»Ÿä¸€ç®¡ç†å¤šå‚å•† AI æœåŠ¡æä¾›å•†
- **[å­æœåŠ¡ç«¯ API](subserver-api.md)** - LangChain + å‘é‡æœåŠ¡ + ä¸ä¸»æœåŠ¡ v3 çš„è¡”æ¥
- **[MCP å®Œæ•´æŒ‡å—](mcp-guide.md)** - MCP å·¥å…·æ³¨å†Œä¸è¿æ¥


