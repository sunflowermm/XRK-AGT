# XRK-AGT Python 子服务端默认配置文件
# 此文件为默认配置模板，首次启动时会自动复制到 data/subserver/config.yaml

# 服务器配置
server:
  host: "0.0.0.0"      # 监听地址
  port: 8000           # 监听端口（默认 8000，避免与主服务端端口冲突）
  reload: false        # 开发模式自动重载（生产环境设为 false）
  log_level: "info"    # 日志级别: debug, info, warning, error

# CORS 配置
cors:
  origins:
    - "*"               # 允许的源（生产环境建议指定具体域名）
    # - "http://localhost:8080"  # 主服务端地址（根据实际端口配置）
    # - "https://yourdomain.com"

# API 配置
api:
  auto_load: true      # 自动加载 apis 目录下的 API
  api_dir: "apis"      # API 目录名称

# 主服务端连接配置（子服务端会调用主服务端的 v3 与 MCP 接口）
main_server:
  host: "127.0.0.1"
  port: 1234           # 主服务端端口（默认 1234）
  timeout: 300         # 请求超时时间（秒）
  api_key: ""          # 主服务端 v3 接口访问鉴权（Bot 启动生成的 apiKey），用于调用 /api/v3/chat/completions

# LangChain 服务配置
langchain:
  enabled: true        # 是否启用 LangChain Agent
  max_steps: 6         # Agent 最大执行步数（低配建议 3-4，高配可 8+）
  max_tools: 40        # 注入给规划模型的工具数量上限（过多会拖慢且更易幻觉）
  verbose: false       # 是否输出详细日志
  request_timeout: 60  # 单次LLM请求超时（秒）

# 代理配置（用于 HuggingFace 模型下载）
proxy:
  http_proxy: ""      # HTTP 代理地址（如: http://127.0.0.1:7890）
  https_proxy: ""     # HTTPS 代理地址（如: http://127.0.0.1:7890）
  hf_endpoint: ""     # HuggingFace 镜像地址（如: https://hf-mirror.com）

# 向量服务配置
vector:
  model: "paraphrase-multilingual-MiniLM-L12-v2"  # 嵌入模型名称
  dimension: 384      # 向量维度
  persist_dir: "data/subserver/vector_db"  # 向量数据库持久化目录
  cache_dir: "data/subserver/model_cache"  # 模型缓存目录（容器内预装模型路径）
  local_files_only: false  # 允许从网络下载模型（本地找不到时自动下载，容器内已预装则优先使用本地）
  load_timeout: 300   # 模型加载超时时间（秒，默认 300，Docker 环境建议增加）
  default_collection: "default"  # 默认collection

# 日志配置
logging:
  level: "info"       # 日志级别
  file: "logs/app.log"  # 日志文件路径
  max_bytes: 10485760  # 单个日志文件最大大小（10MB）
  backup_count: 5     # 保留的日志文件数量
